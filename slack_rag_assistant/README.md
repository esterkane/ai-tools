# Slack RAG Assistant

This is a lightweight Slack app that connects a local LLM (via Ollama, using the Zephyr model) to Slack using the Slack Bolt SDK. It allows users to ask questions directly in Slack and receive answers generated by a retrieval-augmented generation (RAG) backend.

## Features

* Responds to `@mentions` in Slack with context-aware answers
* Integrates with Ollama to use a local Zephyr model
* Provides simple feedback buttons ("Helpful" / "Not Helpful")
* Clean and modular Python codebase
* Uses FAISS and sentence-transformers (optional extension)
* Local knowledge base support using `kb_articles.json`

## Project Structure

```
slack_rag_assistant/
├── app/                     # Application logic
│   ├── __init__.py
│   ├── slack_listener.py     # Handles Slack events
│   ├── embedding_service.py   # Converts text to embeddings
│   ├── vector_store.py        # Manages FAISS index
│   ├── retriever.py           # Implements RAG process
│   ├── llm_interface.py       # Queries the LLM
│   ├── feedback_handler.py    # Manages user feedback
│   ├── slack_ui.py           # Formats Slack messages
│   └── utils.py              # Shared utility functions
├── data/                    # Data storage
│   ├── kb_articles.json      # Mock knowledge base articles
│   ├── index.faiss           # FAISS binary index
│   └── docstore.pkl          # Document ID mapping
├── config/                  # Configuration files
│   ├── config.yaml           # Configuration settings
│   └── prompt_templates.yaml  # Prompt templates for the LLM
├── scripts/                 # Utility scripts
│   └── ingest_docs.py        # Ingests documents and creates FAISS index
├── tests/                   # Test cases
│   └── test_end_to_end.py    # End-to-end tests
├── .env                     # Environment variables
├── requirements.txt         # Project dependencies
├── main.py                  # Entry point for the application
└── README.md                # Project documentation
```

## Setup Instructions

1. **Clone the repository:**

   ```bash
   git clone <repository-url>
   cd slack_rag_assistant
   ```

2. **Create a virtual environment:**

   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows use `venv\Scripts\activate`
   ```

3. **Install dependencies:**

   ```bash
   pip install -r requirements.txt
   ```

4. **Configure environment variables:**
   Create a `.env` file with the following:

   ```env
   SLACK_BOT_TOKEN=xoxb-your-slack-bot-token
   SLACK_APP_TOKEN=xapp-your-slack-app-token
   ```

   Or set them via environment variables.

5. **Prepare the knowledge base:**
   Populate `data/kb_articles.json` with your mock knowledge base articles.

6. **Ingest documents:**
   Run the ingestion script to generate embeddings and create the FAISS index:

   ```bash
   python scripts/ingest_docs.py
   ```

7. **Run Ollama with Zephyr:**
   Install and start the Zephyr model:

   ```bash
   ollama run zephyr
   ```

   Ensure it’s running at [http://localhost:11434](http://localhost:11434).

8. **Start the Slack bot:**

   ```bash
   python -m app.slack_listener
   ```

## Interact

Mention the bot in any Slack channel it’s invited to:

```text
@kb-rag-assistant What is RAG in machine learning?
```

You’ll get a generated answer with feedback buttons.

## Feedback

Button feedback is logged locally. Extend this easily to track analytics, improve answers, or fine-tune models.

## Usage

Once the bot is running, you can interact with it in your Slack workspace. It will listen for messages and respond based on the knowledge base and the context of the conversation.
