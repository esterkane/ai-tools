# Copy this file to config.yaml and adjust paths.

paths:
  books_dir: "./books"
  data_dir: "./data"
  ocr_out_dir: "./data/ocr_out"

qdrant:
  url: "http://localhost:6333"
  collection: "books_chunks"

embedding:
  # For German or multilingual corpora, we recommend a multilingual model (e.g. paraphrase-multilingual-MiniLM-L12-v2)
  model_name_or_path: "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
  device: "cpu"   # use "cuda" if available for faster embeddings

retrieval:
  top_k: 8
  min_score: 0.20
  max_passages: 5
  # language hint for retrieval and claim-check ("de" for German, "en" for English, "auto" to leave as-is)
  language: "auto"
  # Optional rerank config
  # rerank:
  #   enabled: false
  #   model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  #   candidates: 30

chunking:
  max_chars: 2500
  overlap_chars: 200

llm:
  backend: "llama_cpp"   # "llama_cpp" oder "ollama"
  model_path: "./models/your-model.gguf"
  n_ctx: 4096
  temperature: 0.1
  max_tokens: 512

  # ollama (optional):
  # url: "http://localhost:11434"
  # model: "llama3.1:8b"

ui:
  host: "127.0.0.1"
  port: 7860
