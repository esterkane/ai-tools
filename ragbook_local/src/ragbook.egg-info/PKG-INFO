Metadata-Version: 2.4
Name: ragbook
Version: 0.1.0
Summary: Lokales RAG für technische PDFs mit Qdrant + Gradio
Author: brian
License: MIT
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: qdrant-client>=1.9.0
Requires-Dist: gradio>=4.44.0
Requires-Dist: pymupdf>=1.24.0
Requires-Dist: pyyaml>=6.0.1
Requires-Dist: numpy>=1.26.0
Requires-Dist: tqdm>=4.66.0
Requires-Dist: sentence-transformers>=3.0.0
Requires-Dist: rank-bm25>=0.2.2
Requires-Dist: httpx>=0.27.0
Requires-Dist: typer>=0.12.3
Requires-Dist: pydantic>=2.7.0
Provides-Extra: dev
Requires-Dist: ruff>=0.5.0; extra == "dev"
Requires-Dist: pytest>=8.2.0; extra == "dev"
Dynamic: license-file

# ragbook_local — Lokales RAG für technische PDFs (inkl. Scanner-PDFs)

Dieses Projekt ist ein **lokales** (offline-fähiges) RAG-System:
- PDFs (auch gescannte PDFs) werden ingestiert (optional OCR).
- Text wird seitenweise extrahiert, in Chunks gesplittet und in **Qdrant** indexiert.
- Chat-UI (Gradio) beantwortet Fragen **nur** auf Basis gefundener Passagen.
- Bei geringer Evidenz werden **Rückfragen** vorgeschlagen statt zu halluzinieren.
- Die UI zeigt **vollständige Quellen-Passagen** (Highlight/Beleg).

## Schnellstart (empfohlen)
### 1) Qdrant starten (Docker)
```bash
docker compose up -d qdrant
```

### 2) Python-Umgebung
```bash
python -m venv .venv
source .venv/bin/activate
pip install -U pip
pip install -e ".[dev]"
```

### 3) Konfiguration
Kopiere `config.example.yaml` nach `config.yaml` und passe Pfade an.

### 4) Ingest (PDFs indexieren)
```bash
python -m ragbook.cli ingest ./books --config ./config.yaml
```

### 5) UI starten
```bash
python -m ragbook.cli ui --config ./config.yaml
```

## OCR für Scanner-PDFs
OCR ist optional und benötigt System-Dependencies (Tesseract + Ghostscript).
- Linux (Debian/Ubuntu): `sudo apt-get install tesseract-ocr ghostscript`
- Dann:
```bash
python -m ragbook.cli ocr ./books --config ./config.yaml
```
Das schreibt standardmäßig OCR-PDFs in `data/ocr_out/`.

## Hybrid retrieval (BM25 + vector)
A hybrid retrieval mode combines BM25 (lexical) scores with vector similarity from Qdrant. Configure the fusion weight in `config.yaml` under `retrieval.alpha` (float between 0 and 1). The final fused score is:

    fused = alpha * vector_norm + (1 - alpha) * bm25_norm

BM25 uses `rank-bm25` on the stored chunk `text` payloads and runs locally (offline), while vectors are retrieved from Qdrant.

### Optional re-ranking
If you enable `retrieval.rerank.enabled = true`, the system will attempt to load a CrossEncoder from `sentence-transformers` (default: `cross-encoder/ms-marco-MiniLM-L-6-v2`) and re-score the top N candidates (`retrieval.rerank.candidates`). If the model is unavailable or prediction fails, the system logs a warning and proceeds without re-ranking. When re-ranking is applied, the `reason` field returned by the `ChatEngine` will include `(re-ranked)`.

## Lokales LLM
Standard: **llama-cpp-python** (lokal GGUF Model).
- Lege ein GGUF Modell ab (z.B. `models/your-model.gguf`) und trage den Pfad in `config.yaml` ein.

Alternative (optional): Ollama (wenn du es verwenden willst), dann `llm.backend: ollama` setzen.

## Hinweise zu Genauigkeit / Anti-Halluzination
Das System erzwingt:
- Antwort nur, wenn Retrieval-Evidenz über Schwellwert
- Jede Antwort wird mit vollständigen Quellenpassagen begründet
- Andernfalls: "Nicht genug Info" + Rückfragen

## Lizenz
Projekt-Code: MIT (siehe `LICENSE`).
